---
title: "4: Physical Properties of Rivers"
author: "Water Data Analytics | Kateri Salk"
date: "Spring 2022"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Lesson Objectives
1. Compute recurrence intervals for stream discharge 
2. Analyze the effects of watershed disturbance on recurrence intervals and interpret results against the concept of stationarity
3. Communicate findings with peers through oral, visual, and written modes

## Opening Discussion
How is climate change impacting discharge in streams and rivers? What are the new and ongoing challenges faced by these impacts in watersheds?

## Session Set Up

```{r, message = FALSE}
getwd()

# install.packages("lfstat")

library(tidyverse)
library(dataRetrieval)
library(lubridate)
library(lfstat) #low flow statisitcs


theme_set(theme_classic())
```

## Recurrence Intervals and Exceededence Probability

A **recurrence interval** is the past recurrence of an event, in this case a peak annual discharge measurement of a given magnitude. The value of a recurrence interval corresponds to the average number of years between discharge of a given magnitude. Typically the minimum amount of years required to construct a recurrence interval is 10, but 30 is more robust. A recurrence interval, T, is calculated as: 

The dollar signs around an equation, make it appear as an equation when the doc is knitted. Hover over the code here to see how it will knit.
$T = (n+1)/m$

where n is the number of years and m is the ranking of an event within the observed period. We add one to n because we are computing the recurrence interval for a discharge event of a given magnitude *or greater*. 

Similarly, we can calculate an **exceedance probability**, or the probability of encountering a discharge event of a given magnitude or greater in any given year: 

$P = 1/T$

This is where the terms "100-year flood" and similar are derived. Remember this is a probability based on past occurrence, not an accurate forecast of how often we will see that event happening. When current patterns of discharge differ from past patterns, we observe **non-stationary** behavior. Non-stationary results in events that occur more or less frequency than predicted based on the exceedance probability. Stationary means does the behavior of the river change over time? The trend of the river overtime is the same. Can have ups and downs throughout year but not up and down overtime

### Has Eno River dicharge displayed stationary behavior over the period of record?

Let's import discharge data for the Eno River near Durham for all available dates.

```{r}
#End of September is the end of a water year b/c captures end of snow pack and beginning of snow pack in start of year. USGS does this to not split the snowpack.
EnoDischarge <- readNWISdv(siteNumbers = "02085070",
                     parameterCd = "00060", # discharge (ft3/s)
                     startDate = "", 
                     endDate = "2021-09-30")

names(EnoDischarge)[4:5] <- c("Discharge", "Approval.Code")

#gives info from the column
attr(EnoDischarge, "variableInfo")
attr(EnoDischarge, "siteInfo")
  
# Build a ggplot
ggplot(EnoDischarge, aes(x = Date, y = Discharge)) +
  geom_line() +
  labs(x = "Year", y = "Discharge (cfs)")
```

We can then compute recurrence intervals based on the first 30 years of data.

First do manually by data wrangling
in lfstat you have the water_year function to account for other places in the world having different water years
Filter for water year not 1963 b/c 1963 only has a few observations in it
```{r}  
EnoDischarge <- EnoDischarge %>%
  mutate(Year = year(Date), 
         WaterYear = water_year(Date, origin = "usgs")) %>%
  filter(WaterYear != "1963")

# Water Year is a factor. We want to re-classify as numeric. Have to tell R that a factor is a character first and then can make it a numeric from there. If you don't do this it would start by renumbering the years as 1, 2, 3, etc
EnoDischarge$WaterYear <- as.numeric(as.character(EnoDischarge$WaterYear))

#filter for less than 1994 b/c want to do first 30 years of the data
#groupby then summarise does what a pivot table does in excel
#groupby says we want a value for every wateryear. If didn't do this and ran summarise w/o it then summarise would only give one value for all the years together. 
#filter,groupby, and summarise all are intermediates and don't show up in the dataset that is generated. only the mutate is added as a column. if want them added have to do other steps
#summarise is creating a 
#the negative included in rank is telling R to rank the highest value first instead of smallest value as 1
EnoRecurrence <- 
  EnoDischarge %>%
  filter(WaterYear < 1994) %>%
  group_by(WaterYear) %>%
  summarise(PeakDischarge = max(Discharge)) %>% 
  mutate(Rank = rank(-PeakDischarge), 
         RecurrenceInterval = (length(WaterYear) + 1)/Rank, 
         Probability = 1/RecurrenceInterval)
#probability that amount of precip or more occurring


ggplot(EnoRecurrence, aes(x = WaterYear, y = PeakDischarge)) +
  geom_bar(stat = "identity") +
  labs(x = "Year", y = "Peak Discharge (cfs)")

```

Let's display and model the relationship between peak annual discharge and recurrence interval. We can use the statistical model to compute discharge for recurrence intervals that occur above the 30-year mark.
```{r}
#stinkin log10 haha. Log10 makes it look more linear
ggplot(EnoRecurrence, aes(x = RecurrenceInterval, y = PeakDischarge)) +
  geom_point() +
  scale_x_log10() +
  geom_smooth(method = "lm") #, color = "black", se = FALSE)
#change the line color to black and take away the standard error glow

#tells us the equation of the line for the above graph
#91% of variability is accounted for by the model
#can calculate recurrence intervals with as little as 10 years of data but fewer years less accurate
Eno.RImodel <- lm(data = EnoRecurrence, PeakDischarge ~ log10(RecurrenceInterval))
summary(Eno.RImodel)

#What is the discharge for a 100-year flood in this system? a 500-year flood?
#take the intercept (1) and the slope (2) and plug into the equation along with the number of years you're thinking about
#this is telling us, we have the EnoRImodel data and we want to take the coefficient column and the first value in that column and then the second hence the 1 and 2s then log10 for our years. think of the equation from above! See line 122 for the log10(recurrenceinterval)
Eno.RImodel$coefficients[1] + Eno.RImodel$coefficients[2]*log10(100)
Eno.RImodel$coefficients[1] + Eno.RImodel$coefficients[2]*log10(500)
```

What if we were to build a recurrence interval model for the most recent 30 years? How would this compare to the early period recurrence interval?
```{r}
EnoRecurrence.Late <- 
  EnoDischarge %>%
  filter(WaterYear >= 1992) %>%
  group_by(WaterYear) %>%
  summarise(PeakDischarge = max(Discharge)) %>% 
  mutate(Rank = rank(-PeakDischarge), 
         RecurrenceInterval = (length(WaterYear) + 1)/Rank, 
         Probability = 1/RecurrenceInterval)

#As remarkable events (higher recurrence intervals) the larger the discharge for later years than earlier
ggplot(EnoRecurrence, aes(x = RecurrenceInterval, y = PeakDischarge)) +
  geom_point() +
  geom_point(data = EnoRecurrence.Late, color = "#02818a",
             aes(x = RecurrenceInterval, y = PeakDischarge)) +
  scale_x_log10()

#compare our late and early model slopes. big increase in slope for later years
Eno.RImodel.Late <- lm(data = EnoRecurrence.Late, PeakDischarge ~ log10(RecurrenceInterval))
summary(Eno.RImodel.Late)
Eno.RImodel.Late$coefficients
Eno.RImodel$coefficients

Eno.RImodel.Late$coefficients[1] + Eno.RImodel.Late$coefficients[2]*log10(100)
Eno.RImodel.Late$coefficients[1] + Eno.RImodel.Late$coefficients[2]*log10(500)

#can see that the slope is increasing with these results also
#they substansiate. new 100 yr flood is bigger than old 500 yr flood
Eno.RImodel$coefficients[1] + Eno.RImodel$coefficients[2]*log10(100)
Eno.RImodel$coefficients[1] + Eno.RImodel$coefficients[2]*log10(500)

```

What differences did you see for the recurrence intervals built under different periods of record? How would your prediction of flood events differ if you were to use these models for forecasting purposes? 

>The Eno is not displaying stationary behavior. Important to update recurrence intervals as we have more data/ with newer years and also think about impacts of climate change

What would you recommend for a watershed manager seeking to build the most accurate recurrence interval model for the Eno River?

> Use the newer record please.


### Examining the effects of urbanization on discharge

Examples of non-stationary behavior due to human impact

Salado Creek is located in San Antonio, Texas, an area that has been rapidly urbanizing over the course of the last several decades (http://worldpopulationreview.com/us-cities/san-antonio-population/#byPopulation). Is this system exhibiting stationarity?

```{r}
# Import data
SaladoDischarge <- readNWISdv(siteNumbers = "08178700",
                     parameterCd = "00060", # discharge (ft3/s)
                     startDate = "")
names(SaladoDischarge)[4:5] <- c("Discharge", "Approval.Code")
attr(SaladoDischarge, "siteInfo")
  
ggplot(SaladoDischarge, aes(x = Date, y = Discharge)) +
  geom_line() +
  labs(x = "Year", y = "Discharge (cfs)")

#make sure you are calling the correct columns either with names or matrix calls i.e. in da []

#highly urbanized/channelized areas have higher, shorter peaks
#classic example here with higher peaks overtime
```

### Examining the effects of dam construction on recurrence intervals

The stream gage in the Green River near Auburn, Washington, is located directly downstream of the Howard A. Hanson Dam. The dam was built in 1961 for flood control purposes, and the reservoir now provides water supply to the city of Tacoma. How have peak discharges changed since the construction of the dam?

```{r}
GreenDischarge <- readNWISdv(siteNumbers = "12113000",
                     parameterCd = "00060", # discharge (ft3/s)
                     startDate = "")
names(GreenDischarge)[4:5] <- c("Discharge", "Approval.Code")
attr(GreenDischarge, "siteInfo")
  
ggplot(GreenDischarge, aes(x = Date, y = Discharge)) +
  geom_line() +
  labs(x = "Year", y = "Discharge (cfs)")

#downstream from Howard Hanson dam. built in 1961. can see the immediate drop in the peak. built for flood control for Tacoma, Washington
```

## Bonus content: Flow Duration Curves and Low Flow Statistics

Flow-duration curves can be generated from daily discharge data, similar to how we calculated recurrence intervals for annual data. 

$P = 100*(m/(n+1))$

where P is the exceedance probability, m is the ranking of all daily mean flows in the period of record (at least 10 years), and n is the total number of daily mean flows. 

We focused today on recurrence intervals, which use peak flow statistics. On the other end of the discharge gradient are low flow statistics, most commonly estimated by 7Q2 and 7Q10 metrics (7-day, 2-year and 10-year annual low flow statistics). These can be used to evaluate drought conditions and are another metric for evaluating stationarity in rivers and streams. 

See the USGS description of these statistics here:(Calculating Flow-Duration and Low-Flow Frequency Statistics at Streamflow-Gaging Stations)[https://pubs.usgs.gov/sir/2008/5126/section3.html]
